\chapter{General Discussion}\thumbforchapter
\newpage

All cells in our body contain the same DNA, yet the cells in our liver express a completely different set of genes than the cells in our heart. Gene expression regulation is the mechanism that controls the activation and repression of genes in the genome. The spatial and temporal control of gene regulation is an essential part of (embryonic) development. Moreover, genes and their spatio-temporal expression patterns and regulation are remarkably conserved between species.

In this thesis, I studied gene expression regulation from an evolutionary-developmental point of view. I first discussed three recommendations for gene regulatory network inference in development; multi-omics data, single-cell sequencing, and artificial neural nets (\textbf{Chapter 2}). Next, I presented seq2science, a preprocessing workflow for functional genomics analysis (\textbf{Chapter 3}). This chapter is followed by a description of how the definition of the phylotypic stage is ambiguous and how this ambiguity leads to the development of new statistical controls. Applying these new controls to previous studies exposes flaws in the methodology and the subsequent interpretation of their results (\textbf{Chapters 4 and 5}). Finally, in \textbf{Chapter 6}, I presented a novel method to infer transcription factor activity based on single-cell transcriptomic data. Here I present my final concluding remarks.

\section{Inferring gene regulatory networks}

Gene regulatory network inference is the computational and statistical approach to deciphering the regulatory interactions between genes. Understanding how certain gene products, for instance, transcription factors, regulate the expression of other genes helps with gaining a fundamental understanding of cellular processes. It has become a standard analysis step in most (single-cell) studies, however, the field seems to dismiss the fact that the inferred networks perform barely any better than random networks\cite{McCalla_2021,Chen_2018,Pratapa_2020}. In \textbf{Chapter 2} I discuss three (potential) improvements over "traditional" gene regulatory network inference; the inclusion of multi-omics data, single-cell sequencing, and artificial neural networks.

As a first recommendation, I suggest the inclusion of multiple sequencing modalities. Historically, gene regulatory networks have mainly been inferred based on transcriptomic data alone. Apart from the issue that these networks perform poorly\cite{McCalla_2021,Chen_2018,Pratapa_2020}, there is the fundamental issue of using transcripts both as input and as output in these models. Transcript abundance is the result of transcription regulation and degradation. Transcripts, however, are typically not responsible for signal transduction, chromatin remodeling, translation, and transcription regulation and degradation. The protein product synthesized from these transcripts is responsible for these types of regulation and is assumed to correlate closely with transcript abundance. The correlation coefficient between transcript and protein, however, ranges between 0.3-0.4\cite{Fortelny2017,Franks2017}, with transcript abundance thus approximately explaining only between 0.09-0.16 of the variance of protein abundance. Similarly, in \textbf{Chapter 6} I show the poor correlation between transcription factor expression level and motif activity. High-throughput proteomics is more costly than sequencing data and lacks its accuracy, with single-cell proteomics lagging behind single-cell transcriptomic and (epi)genomic techniques\cite{Bennett2023}. More accessible and accurate metrics for gene regulation are genomic assays that measure genome accessibility and histone modifications\cite{Xu_2020,Kamal_2021,Aibar_2017}, which in turn improve the performance of these models. Ideally, future GRN inference methods include (epi)genomic, transcriptomic, and proteomic information, and using only transcriptomic data was perhaps in hindsight a naive endeavor.

The second recommendation I suggest is the use of single-cell data. Single-cell sequencing is important because it allows for the separation of the different signals present in tissues. Moreover, by separating cells in samples it is possible to order them on a fine-grained (pseudo)time\cite{Saelens2019}, allowing for new ways to study this problem and increasing the detail of these analyses. The analysis of single-cell data however has turned out to be problematic with major issues of batch effects \cite{Tran2020,Haghverdi2018,Lhnemann2020} and sparsity\cite{Lhnemann2020,Bouland2023}. Furthermore, gene regulatory networks inference methods specifically designed for single-cell transcriptomic data are actually not more accurate than their "bulk" counterparts\cite{Chen_2018}. So whilst single-cell data has major potential to improve GRN inference, in practice, so far the provided improvements to gene regulatory network inference have been marginal. 

The final recommendation I make is the use of artificial neural networks, which is a recommendation I would like to redefine here. One of the reasons I recommended the use of ANNs is because they can incorporate and train on data over multiple conditions. The networks that come out of these models can thus be based on a wide range of conditions, and are known as foundation models. As long as enough data is used we can expect these foundation-model-based networks to be able to extrapolate to new unseen conditions\cite{Schreiber2020_avocado}. This makes these types of networks powerful methods for hypothesis generation, especially compared to the usual approach of inferring gene regulatory networks based on the difference between two conditions. However, ANNs are not the only type of method that can produce foundation models. For instance, one can develop (mechanistic) gene networks based on ordinary differential equations that model changes in gene expression over time\cite{Ventre_2022}. These types of "universal" networks make conceptually the most sense, as ultimately, a single set of instructions (DNA) is shared across all cells of an individual. As such I would redefine the last recommendation to be to design foundation gene networks, with the caveat that current foundation models fail to outperform their simpler counterparts\cite{Kedzierska2023}.

In \textbf{Chapter 6} I introduce the computational tool scepia, which infers differentially active transcription factors in single-cell transcriptomic data. It matches transcriptomic data to a reference H3K27ac database. In this manner, the transcriptomic data can be used to find differential transcription factors and link them to the differential motifs in the reference database. Even though the basic assumption of scepia, the linking of epigenomic data to transcriptomic data by regulatory potential\cite{Wang2016}, is relatively simple, it seems to perform reasonably well. Nonetheless, the approach of scepia contains two major downsides. First, it produces combinations of differential transcription factors and motifs, but generates no potential gene regulatory network. This means that it is difficult to infer cause-and-effect relationships based on the output of scepia. Second, the implementation is based on numerous assumptions and heuristics. We have limited understanding which of these heuristics are beneficial or detrimental to its performance, and as such we have not gained a new fundamental understanding of which types of interactions are important or how they interact. Moreover, due to the lack of a benchmark against the alternatives to scepia\cite{Aibar_2017,Dong2022}, it is unclear how this approach compares to alternative methods. Ideally, any new gene regulatory network inference method performs an extensive parameter sweep over all design choices as in \cite{Gschwind2023}. Nevertheless, this might not always be feasible, because of computational constraints and the lack of a gold standard.

Finally, the gene regulatory network field lacks a formal language by which to describe gene interactions\cite{Lazebnik2002}. Studies presenting their new methodology usually report a complex graph of nodes (genes) and vertices (their interactions)\cite{Xu_2020,Aibar_2017,Margolin_2006,Glass_2013,Chan_2017,Jiang_2021,Kamimoto_2020,Woodhouse_2018,Rubiolo_2017}. It is not uncommon for studies to report figures with dozens of genes connected by hundreds of overlapping lines. Moreover, what these lines, and their color and thickness, mean is decided on a case-by-case basis. Understanding and remembering these networks poses a challenge. It has often been suggested to outsource this complexity to computational biologists\cite{Lazebnik2002,Bray2001,Markowetz2017}. Whilst I agree with the general tendency that all biologists should learn to program, I think a better solution is possible. During my doctoral studies, I have always looked with envy at the networks that the fields of metabolomics and electrical engineering produce. These fields have managed to condense extremely complex interactions into organized and even visually pleasing networks. I believe the difference lies in their ability to formally simplify their interactions and subnetworks. For instance, in major metabolomic pathway visualizations, not all the interactions are shown, but for example, a circle is drawn to represent the cyclic acid cycle. When one is specifically interested in the cyclic acid cycle one can then look this cycle up in another diagram. Similarly, all standard components of electrical networks, such as batteries, capacitors, and resistors, are defined by the International Electrotechnical Commission\cite{IEC}. As far as I am aware, the only systematic approach to GRN visualization is BioTapestry\cite{Longabaugh2011}, but this visualization tool has not gained a substantial foothold in the field. To gain a comprehensive understanding of gene networks, the field has to adopt the practices of modular subnetworks and standardized symbols. Without these changes, the interpretation and analysis of gene networks will remain limited to computers and computational biologists exclusively.

\section{The scientific dogma of the phylotypic stage}

\begin{shadequote}[c]{George Box}
All models are wrong, but some are useful.
\end{shadequote}

While historically the phylotypic stage has predominantly been examined and described through qualitative methods, the 21\textsuperscript{st} century started a paradigm shift towards a more quantitative and data-driven approach to understanding this phenomenon\cite{Chan2021}. The first notable quantitative investigation into the phylotypic stage was done by Bininda-Emonds \textit{et al.}, where they calculated temporal conservation as the order in which morphological embryonic features appear in vertebrates\cite{OlafRP2003}. However, it wasn't until the early 2010s that the field truly embraced quantitative methodologies with the simultaneous publication of two groundbreaking studies in Nature\cite{Kalinka2010, DomazetLoso2010}. In these works, Domazet-Lošo \textit{et al.} investigated the average developmental age of transcripts in \textit{D. rerio} and \textit{D. melanogaster}, whilst at the same time, Kalinka \textit{et al.} explored the temporal transcriptome similarities across different \textit{Drosophila} species. These molecular studies opened a new line of research to the quantitative basis of the phylotypic stage. The quantitative support for the phylotypic stage appears stronger and stronger with each new study. So strong, that we quickly forget all the nonconforming results.

The Transcriptome Age Index (TAI), as introduced by Domazet-Lošo \textit{et al.}, is a metric of the average evolutionary age of transcripts over time\cite{DomazetLoso2010}. In this study, evolutionary age is determined as the count of taxonomic branches that can be traced back to a gene. The central idea of the TAI is that temporal changes in the average transcript age provide insights into the degree of conservation during development. Domazet-Lošo \textit{et al.} found that both zebrafish and \textit{Drosophila} expressed the oldest transcriptome at their respective phylotypic stages and concluded that an old transcriptome marks the phylotypic phase. However, an independent re-analysis conducted by Piasecka \textit{et al.} raised some critical points about the methodology\cite{Piasecka2013}. Their investigation revealed that the TAI is heavily influenced by a relatively small subset of genes due to major differences in transcript levels per gene (transcriptomic data is notoriously heteroscedastic\cite{Rocke2001}). Log transforming the data, which is a standard processing step for this type of data, completely invalidates the results of the original study. One might expect such a dependency on data transformation to cast doubts on the reliability of the method. Surprisingly, the opposite appears to be true. The original study introducing the TAI has been cited 88 times between 2010 and 2013, but has been cited 359 times since Piasecka \textit{et al.}'s publication (covering the years 2014-2023). As it turns out, you can now analyze the data with and without transformation, and keep the results that reinforce your preferred hypothesis. A notable example of this is Wu \textit{et al.}'s study on Spiralian development\cite{Wu2019}. In their analysis of untransformed data for \textit{Crassostrea gigas}, \textit{Haliotis discus hannai}, and \textit{Perinereis aibuhitensis}, they claim to have found an inverse hourglass pattern of evolutionary conservation and speculate why spiralia have a different temporal selection pressure than other species. However, their supplementary data reveals a different pattern for \textit{Crassostrea gigas} after square root transformation, shifting from an inverse hourglass to a funnel shape. Remarkably, this crucial finding receives minimal attention in the study, with the authors merely stating that at least the transformed data does not show an hourglass-like pattern. Moreover, the transformed TAI of the other two species is not even shown, and upon closer inspection, it cannot be excluded that the inverse hourglass pattern of \textit{Perinereis aibuhitensis} on untransformed data is caused by stochastic fluctuations. Where one should be careful with elaborate interpretations of this study, it has instead sparked an exchange among three influential evolutionary-developmental biologists - Pavel Tomanczak, Denis Duboule, and Andreas Hejnol - on Twitter (now X), about the universality of the hourglass model\cite{hejnoltwitter}. It is worth mentioning that Andreas Hejnol has authored two critical reviews about the methodology and design of previous studies that asserted the universality of the phylotypic stage\cite{Dunn2018,hejnol2016}.

The work of Barbara Piasecka, where she showed that the pattern of the TAI is caused by a subset of all genes was led by Marc Robinson-Rechavi. The main work of this study was not about the TAI, but about using a multitude of metrics to estimate temporal evolutionary conservation. They conclude that different metrics produce different results. In his personal blog, Marc Robinson-Rechavi states \say{First, that biology is complicated, and insisting on answers such as « the hourglass exists (and explains diverse data) » or « it doesn’t » may not be the best strategy. Second, that the technical details are very important}\cite{robinsonrechaviblog}. Marc Robinson-Rechavi's later career, however, appears to have diverged from his earlier conclusions. He has made assertive claims concerning the ortholog conjecture\cite{KryuchkovaMostacci2016} and the hourglass model of conservation\cite{Liu2020,Liu2021,marletaz2018}. A re-analysis by Casey Dunn, Andreas Hejnol, and others identified methodological issues with their analysis of the ortholog conjecture\cite{Dunn2018}, and in \textbf{chapter 4} I discuss in detail the methodological problems of two of his studies related to the hourglass model.

In 2003, Bininda-Emonds \textit{et al.} conducted a quantitative study of the phylotypic stage, which was revisited seventeen years later by Gerardo A. Cordero \textit{et al}\cite{OlafRP2003, Cordero2020}. Both studies were about the quantitative temporal analysis of morphological characteristics. To the best of my knowledge, these are the only quantitative analyses of morphological characteristics concerning the phylotypic stage. The initial findings of Bininda-Emonds \textit{et al}. revealed an unexpected inverse hourglass pattern in morphological rankings, a discovery that challenged the existing assumption of mid-development being the most conserved period. It took seventeen years for a follow-up study, that surprisingly showed precisely the opposite - an hourglass pattern. Unfortunately, Cordero \textit{et al.} only comment that the difference between these two studies \textit{could} be caused by a difference in morphological characteristics, methodology, and species used, without any further analysis of the differences. They were correct in their assessment however, as in \textbf{chapter 5} I show through simulations that the inverse hourglass pattern of Bininda-Emonds \textit{et al.} is likely caused by the methodology's sensitivity to edge effects, and does not represent a biological signal.

In 2016, Levin \textit{et al.} introduced the transcriptomic inverse hourglass model as a potential method for distinguishing between different phyla\cite{Levin2016}. However, this study has been criticized by Casey Dunn and Andreas Hejnol for its lack of a within-phylum control\cite{hejnol2016} and incorrect statistical methods\cite{Dunn2018}. Given the ambitious claim of a "universal" phylotypic stage characterized by a high similarity within phyla but a low similarity between phyla, it is somewhat perplexing that these criticisms have not yet been addressed by Levin \textit{et al}. What makes the situation even more confusing is that another group of evolutionary developmental biologists compared the embryonic development of deuterostomes and the chordate amphioxus - a between-phyla comparison. Astonishingly, they uncovered an hourglass-like pattern\cite{PerezPosada2022}, directly contradicting Levin \textit{et al.}'s inverse hourglass model, but do not comment on this. In \textbf{Chapter 4}, I present evidence that the transcriptomic inverse hourglass is a statistical artifact resulting from normalization rather than an accurate representation of temporal conservation. 

Yoav Mayshar \textit{et al.} studied the phylotypic stage and the hourglass model from a single-cell point of view\cite{Mayshar2023}. Their research involved a comparative analysis of cell type proportions during the development of rabbit and mouse embryos. However, in \textbf{Chapter 4}, I show that both the rabbit time series as well as the mouse time series exhibit discontinuous patterns. These discontinuities influence the temporal correlations between the two species. Without a better distributed temporal sampling, a direct comparison with this data concerning temporal conservation is not possible. Moreover, when there is a mid-developmental transition between two time series, pairwise comparisons between these two time series are visually similar to an hourglass. Thus confusingly, when comparing two time series, if the data visualization shows an hourglass-like pattern, this represents the inverse hourglass model. It seems that Mayshar \textit{et al.} fell for this pitfall, as they perceive their mid-developmental transition as confirming the traditional hourglass model. Perhaps even more surprising, is that no one corrected their misinterpretation when they published their preprint on Biorxiv and that the reviewers of Cell, one of the most prestigious journals, failed to notice it.

\begin{figure}
    \includegraphics[width=\linewidth]{ch.discussion/imgs/dogma.png}
    \caption{\textbf{The phylotypic stage as a (crushing) scientific dogma?} \cite{Caveman2000}.}
    \label{fig:dogma}
\end{figure}

These examples highlight the dogmatic belief in the (non)existence of a phylotypic stage, and a reluctance to sincerely interact with each other's works. On top of that, in \textbf{chapter 4 and 5}, I discuss the methodological issues in comparative analyses concerning the phylotypic stage. The root problem is that the phylotypic stage and its related models are ill-defined, and as a consequence, there is a concerning lack of appropriate (statistical) controls in these studies. Is temporal conservation in these models present in comparisons within and between phyla? And is temporal conservation already observable when comparing a species against itself? By systematically addressing these ambiguities in previous studies through within-species, between-species, and between-phyla comparisons I show examples where the conclusions are not supported by the data. To summarize my main findings:
\begin{itemize}
    \item The transcriptomic hourglass-like pattern between zebrafish and frogs\cite{marletaz2018} can be explained by within-species correlations alone.
    \item The transcriptomic between-phyla inverse hourglass pattern\cite{Levin2016} is a statistical artifact and can be reproduced by simulated data with no specific temporal conservation.
    \item The pattern of cell type proportion similarity between rabbits and mice\cite{Mayshar2023} can be explained by discontinuous temporal sampling.
    \item The \textit{Drosophila} enhancer conservation re-analysis results in a "mid-developmental" stage of maximum similarity, albeit at a different point than found by the original authors. Moreover, in \textbf{Chapter X} I highlight additional problems with the binarization of enhancers, and as such I do not endorse this methodology.
    \item  The morphological within-phylum inverse hourglass pattern is caused by edge effects and can be reproduced by simulated data with no specific temporal conservation.
\end{itemize}
\noindent
The null model for evolutionary embryonic development would be that there is no specific stage of higher or lower temporal conservation. Altogether, I have found little evidence to reject the null hypothesis of constant temporal conservation based on quantitative data, and thus see no reason to believe in a (molecular) phylotypic stage.

Moreover, in \textbf{Chapter 4} I discuss further ambiguities in the models of the phylotypic stage. If a molecular phylotypic stage would exist, which features are expected to be conserved and which are not? The original observation that vertebrate embryos, perhaps, look more like each other \textit{externally} at certain points in development, says little about the whole-embryo molecular basis for it. While the morphological observations of Haeckel are almost 200 years old, there have been only two quantitative studies about the morphological phylotypic stage, and these two studies are in direct contradiction with each other. Instead, there appears to be a pursuit to find new (quantitative) methodologies that produce a more straight-forward confirmation of the existence of the phylotypic stage, such as embryonic lethality\cite{Uchida2018}, morphology\cite{OlafRP2003,Cordero2020}, DNA sequence conservation\cite{Piasecka2013,Quint2012,Liu2021} and activation order\cite{Uesaka2019}, cell type proportion\cite{Mayshar2023}, and whole-transcriptome similarity\cite{Piasecka2013,Irie2011,marletaz2018,Liu2020,Leong2021,PerezPosada2022,Kalinka2010}, with little sincere effort to integrate these results across studies. Of these methodologies, the whole-embryo transcriptome has become the most popular method to asses quantitative similarity. The implicit justification for this is that the transcriptome is an unbiased way to study genes during development. Yet, the more cynical interpretation is that whole-embryo transcriptomic studies are the most confirming of our prior beliefs about the phylotypic stage. Even assuming the phylotypic stage exists, why would we expect to be able to measure such a complex phenomenon with such crude methods as observational studies and whole embryo sequencing? 

Throughout scientific history, certain theories, such as taxonomic phyla and the phylotypic stage, have evolved from initial concepts into widely accepted truths, creating a demand for a molecular explanation along the way. However, a fundamental issue arises from the loose and ambiguous definitions on which these theories are based, leading to their lack of predictive power and falsifiability, rendering them, by Popperian standards, non-scientific in nature. For instance, the concept of phyla hinges on the notion that animals sharing a common basic body plan are part of the same phylum, yet paradoxically, the basic body plan is defined as the morphological characteristics shared by all animals within the same phyla\cite{BUDD2000,scholtz2004bauplane}. The definition of the phylotypic stage is similarly ambiguous. Historically, the pharyngula stage\cite{BALLARD1981}, early somite embryo\cite{Alberch1993}, and the tail bud stage \cite{Slack1993} have all been proposed as the vertebrate phylotypic stage. In quantitative studies, the choice of definition in turn depends on which stage exhibits the highest quantitative conservation. Consequently, the pharyngula \cite{Irie2011,marletaz2018}, the early somite embryo \cite{DomazetLoso2010}, or simply the stage(s) with the highest conservation metric\cite{Kalinka2010,Cordero2020} have all been identified as phylotypic stages. Our current approach to studying the phylotypic stage, where we selectively include definitions and ignore non-conforming results, is not only wrong, but also not useful.

\section{The technical debt of bioinformatics}

The complexity of molecular biological systems naturally makes their bioinformatic analysis a complex task as well. This is further complicated by the continuous addition of new insights, which leads to updated methodologies and analytical approaches, keeping it in a state of perpetual change. Often in the rush to generate new insights, tools are developed too quickly, which results in solutions that are not fully thought-through and oversimplify the actual biology. This in turn hinders later analyses, which continue on these suboptimal solutions. In software development, this problem is known as "technical debt". Technical debt is analogous to financial debt, where you borrow money and must eventually pay it back, including interest. In the case of bioinformatics, the debt represents unfit data structures, unoptimized tools and algorithms, and the interest to be paid is increased workload and worse results, I believe that in the field of bioinformatics, the technical debt has become so large that it has become prohibitive for analyses.

\bigbreak
A personal example of how technical debt influenced my research is the Sequence Read Archive.

The Sequence Read Archive (SRA) is a public repository that serves as a centralized archive for storing and sharing high-throughput sequencing data. Its primary goal is to facilitate the sharing and accessibility of raw sequence data produced by various genomics and transcriptomics experiments. The SRA has been essential for my doctoral studies, as I have relied solely on public data. For \textbf{Chapter 6}, I aimed to download all available human H3K27ac data (+/- 12,000 samples) from the SRA as a reference database. Processing these samples meant that I had to download 20TB from the SRA and spent approximately 300,000 CPU hours processing them on Cartesius, costing an enormous amount of computational resources and emitting roughly one-and-a-half tonnes of CO\textsubscript{2}\footnote[2]{300,000 CPU hours, spread over 8 cores of an Intel Xeon Processor E5-2670 uses 115W\cite{intelIntelXeon}, results in an energy usage of 4312 kWh. The Netherlands emitted an average of 369 grams of CO\textsubscript{2} per kWh\cite{CO2} in 2019, totaling 1591 kg.}. In the analysis that followed, however, it turned out that obtaining sample annotations, such as tissue or cell type, is challenging due to the lack of standardized metadata on the SRA. Even though third-party tools like MetaSRA\cite{Bernstein2017}, and PredictMEE\cite{Klie2021} have been developed to automatically infer this metadata, they did not manage to resolve my issues. In the end, I decided to discard the experiment, wasting an enormous amount of resources.

Not only is the SRA missing crucial metadata, but their sra-tools, a toolkit for downloading sequencing data, are also particularly difficult to use. As a consequence, many alternative tools have been developed for downloading data from the SRA, such as the SRA-explorer\cite{sraexplorer}, pysradb\cite{pysradb}, FetchFastQ\cite{galvez2022metadata}, nf-core/fetchngs\cite{fetchngs}, parallel-fastq-dump\cite{parallelfastq}, and finally our download-fastq workflow of seq2science\cite{seq2science} (\textbf{Chapter 3}). The poor implementation of the sra-tools results in additional and unnecessary work, costing time and resources of third-party researchers. Similarly, the submission of new sequencing data is notoriously complicated, causing people to make mistakes, which even leads to others developing tools to streamline this process\cite{Quiones2020}. 

\bigbreak
During my doctoral studies, I encountered several examples of oversimplified biological data representation, particularly in the use of Position Weight Matrices (PWMs) the fasta format for whole genomes, and positional genomic formats such as the GFF and GTF formats. PWMs, which are essentially scoring matrices, quantify the log-likelihood of each nucleotide's presence in a motif. While PWMs are straightforward to understand and visually interpret, they oversimplify transcription factor (TF) binding by assuming that nucleotides within a binding site act independently, which is not true\cite{Zuo2014,Jolma2015,Luscombe2001}. This oversimplification results in motif databases consisting of numerous nearly identical PWMs. A remarkable example is the SCENIC+ motif database\cite{BravoGonzlezBlas2023}, which claims to have compiled the largest collection of motifs to date, boasting over 30,000 "unique" motifs. However, these motifs are distributed among approximately 3,000 TFs, demonstrating a considerable amount of redundancy. 

Similar issues exist for the FASTA file format when used for whole genomes\cite{Lipman1985}. FASTA is a text-based format that represents nucleotide sequences or amino acid sequences, in which nucleotides and amino acids are represented using single-letter IUPAC codes. The FASTA format, however, can not naturally represent uncertainty at genomic locations. Single nucleotide polymorphisms are defined by their IUPAC codes, but uncertainties longer than a single nucleotide can not be naturally represented. The solution for this problem has been to append alternative sequences to the file. These sequences can then selectively be used in the downstream analysis. However, bwa mem(2)\cite{bwamem,bwamem2}, and Illumina Dragen are the only genomic aligners that currently support this natively, even though alternative regions have been added to the human genome since 2013. 

Similarly, the General Feature Format (GFF), and its derived Gene Transfer Format (GTF) are formats that are not fit for their use anymore. The GFF and GTF format specify genomic features and their locations, where each line represents a feature and the columns are different attributes. One of these columns is the one-indexed position of the feature, even though it is the norm (this is opinionated) for coordinate systems to be zero-indexed\cite{utexasEWDijkstra}. This makes one-off errors between these files and other positional formats such as BED easy to make. Moreover, as our understanding of genes and thus their subsequent analysis got more complex, it became necessary to express more complex relationships between features within these files. Line-based formats such as GTF and GFF can not naturally represent this, yet the newer GFF and GTF formats now specify these relationships. This has made these formats less suitable to be parsed line-by-line, something for which they were originally designed. Even the first sentence of the GFF3 format starts disagreeing with this design; \say{although there are many richer ways of representing genomic features ..., the stubborn persistence ... of ad-hoc tab-delimited flat file formats declares the bioinformatics community's need for a simple format that can be modified with a text editor and processed with shell tools like grep}. Moreover, the unstructured manner in which these new formats were introduced has led to multiple incompatible dialects. The frustration of the field about the GFF and GTF formats is best illustrated by a Tweet of Lior Pachter, where he asked if there is a citation for the GTF format. The most popular response covered the collective frustration of the field, stating \say{Ain’t no one gonna take responsibility for that mess}.
% TODO CITATION https://twitter.com/lpachter/status/1719492098771300536
% https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md

The field of bioinformatics shows a clear reluctance to move beyond established, yet increasingly inadequate data representation models. It seems unavoidable that in the near future (i) transcription factor motifs will mainly be defined by higher-order representations, such as graph-based PWMs\cite{Siebert2016} or neural networks\cite{Novakovsky2023,https://doi.org/10.48550/arxiv.1704.02685}, (ii) genome assemblies will be pangenomes, and (iii) the GFF format is replaced by a graph-based format. Adoption of these new formats is no easy task, but at some point has to be done. And until that time the field must pay the price by bad analyses.

\bigbreak
Finally, the need to develop fast.  More confusing than the outdated fasta format is the slow adoption of updated genome assemblies. For example, in 2023, Google Scholar identified 7,630 publications that utilized the older hg19 or grch37 genome assemblies (released in 2009), while there were only 8,960 publications that used the newer hg38 or grch38 assemblies (released in 2013). Notably, the T2T-CHM13 assembly (released in 2022) has only received 222 citations in 2023. A better genome assembly, not surprisingly, results in more accurate clinical discoveries\cite{Aganezov2022}. Ultimately, the slow adoption of updated genomes by the community and the limitations of the fasta format inhibit discoveries in the field of bioinformatics. Single-cell tools are usually implemented poorly, taking days to run and requiring enormous amounts of memory\cite{Pratapa_2020}. The usual solution to this problem is not to improve the software implementation, but instead to increase the hardware capabilities. For instance, an ex-colleague of mine requested a massive 4TB of RAM for his computer at his new position. A particularly painful point of technical debt in the single-cell field is the implementation of the log fold change calculation of Seurat. Seurat calculates the log of the means (as opposed to the mean of the logs), which makes it sensitive to outliers and this implementation should be corrected. This results in vastly different outcomes depending on whether one used Seurat or any alternative tool such as Scanpy for their analysis. The bug was reported in November 2022, got an incomplete fix in March 2023, and as yet has not been fixed (https://github.com/satijalab/seurat/issues/6654). Seurat has been cited 7,699 times.


Researchers have to develop new tools and analyses, whilst it is not even clear whether those approaches will be successful. In these cases taking \textit{deliberate} technical debt is okay. The problem with bioinformatics is, however, that a large part of the technical debt is taken inadvertently. I think the three main reasons for this are the lack of formal bioinformatics training (as mentioned in \textbf{chapter 2}), the lack of interdepartmental software collaborations, and the lack of incentives. 

% The debt has to be paid!

Because the field of bioinformatics is young and rapidly becoming more important the supply of bioinformaticians is lacking behind their demand. This in turn leads to biologists starting bioinformatics analyses without formal training. This is somehow a widely accepted fact in the field, and even facilitated and encouraged by projects such as Galaxy\cite{galaxy}. In turn, bioinformaticians without formal training in programming, statistics, or modeling are developing tools and file formats, and doing analyses, which simply means that the quality and correctness of these tools and analyses are low. A clear example is the widespread use of Excel, which notoriously mangles gene names\cite{Zeeberg2004}. For example, the gene SEPT1 gets converted into September 1, and consequently, approximately $30\%$ of studies report these mangled gene names in their supplementary data\cite{Abeysooriya2021}. Likewise, as far as I am aware all the first authors of the studies I re-analyzed in \textbf{Chapters 4 and 5} are primarily trained as biologist, not as computational biologists. It is surprising that the idea prevails that anyone should be able to do a bioinformatics analysis, whilst wet lab work is typically restricted to those who have undergone formal training. By not formally training the current and next generation of bioinformatics, we are hindering long-term progress in the field.

Another part of the problem is that most tools are developed independently, most of the time by a single doctoral student or postdoc. This has two major downsides. The first is that the maintenance of tools depends on individuals. If someone decides to leave academia, their tools, how useful they might be, will soon become obsolete as no one will maintain them. A clear example is the dissolution of the \say{van Heeringen} group, where soon tools such as genomepy\cite{genomepy}, gimmemotifs\cite{Bruse_2018}, ananse\cite{Xu_2020}, scepia, seq2science\cite{seq2science}, and qnorm\cite{qnorm} will become obsolete as they will no longer be maintained. Second, most tools are unnecessarily specialized for the task the developer is trying to solve. This in turn leads to many near-identical implementations that solve the same problem, for example, the many similar SRA-fastq downloading implementations mentioned before, including the seq2science implementation. These problems could be overcome if bioinformaticians would develop more interdepartmental collaborations. A software collaboration between multiple groups and expertise levels would ensure projects are not abandoned when a person leaves and that tools are useful for wider audiences. Moreover, working in close collaborations has the advantage that people learn from one another.

In summary, despite the remarkable scientific breakthroughs facilitated by bioinformatics in the past decades, the field is confronted with an increasingly taxing technical debt. This accumulating burden reduces the efficiency of current analyses, increases the risk of errors, and demands urgent attention. Addressing bioinformatics' technical debt requires a normalization of formal training and interdisciplinary collaborations - failure to do so risks inhibiting future advancements in the field.




% Biological systems are inherently complex, which in turn makes their (bioinformatics) analysis complicated. The complexity of these analyses is worsened by the continuous addition of new technologies and insights. However, unnecessarily adding to this complexity is the use of outdated and idiosyncratic software and file formats. This in turn leads bioinformaticians to more easily make mistakes and do duplicate work. In software development, this problem is known as "technical debt". Technical debt is analogous to financial debt, where you borrow money and must eventually pay it back, including interest. I believe that in the field of bioinformatics, the technical debt has become so large that it has become prohibitive for analyses.




The most common file formats in bioinformatics were designed in the 90s and 00s, reflecting the tools of their time. Perl and awk were popular languages for bioinformatics, and for this reason, file formats are mostly plain text and line-based. The sam, bam, bed, gff, and vcf file formats all specify genomic features and their location, and are a great example of ill-thought-through design. The bam- and bed-format are zero-indexed\cite{Li2009}, whilst the sam-, gff-, and vcf-format are one-indexed\cite{Li2009,Danecek2011}. This makes one-off errors between these formats extremely easy and incurs significant development time for anyone working with these formats. Moreover, as our understanding of genes and their analyses got more complex, it became necessary to express more complex relationships between features within these files. This has led to a multitude of similar but incompatible gff versions and formats. The addition of these more complex relationships in gff files has made the format less suitable to be parsed line-by-line, which the format was originally designed for. An illustrative example highlighting the issues with positional formats is a tweet by Lior Pachter, where he asked if there is a citation for the gtf format. The most popular response covered the collective frustration of the field, stating \say{Ain’t no one gonna take responsibility for that mess}.
% TODO CITATION https://twitter.com/lpachter/status/1719492098771300536

Another example of an outdated file format in bioinformatics is the fasta file format\cite{Lipman1985}. The fasta file format, which was developed in 1985, is a text-based format that represents nucleotide sequences or amino acid sequences, in which nucleotides and amino acids are represented using single-letter IUPAC codes. Its age initially shows because lines are typically split over 80 characters, as that was the maximum screen width of the past. The true problem with the fasta format is, however, that it can not naturally represent uncertainty at genomic locations. Single nucleotide polymorphisms are defined by their IUPAC codes, but uncertainties longer than a single nucleotide can not be naturally represented. The solution for this problem has been to append alternative sequences to the fasta. These sequences can then selectively be used in the downstream analysis. However, bwa mem(2)\cite{bwamem,bwamem2}, and Illumina Dragen are the only genomic aligners that currently support this natively, even though alternative regions have been added to the human genome since 2013. A switch to a graph-based genome format\cite{Li2020} is obvious and seems unavoidable, but has somehow been postponed by the bioinformatics community. More confusing than the outdated fasta format is the slow adoption of updated genome assemblies. For example, in 2023, Google Scholar identified 7,630 publications that utilized the older hg19 or grch37 genome assemblies (released in 2009), while there were only 8,960 publications that used the newer hg38 or grch38 assemblies (released in 2013). Notably, the T2T-CHM13 assembly (released in 2022) has only received 222 citations in 2023. A better genome assembly, not surprisingly, results in more accurate clinical discoveries\cite{Aganezov2022}. Ultimately, the slow adoption of updated genomes by the community and the limitations of the fasta format inhibit discoveries in the field of bioinformatics.

The Sequence Read Archive (SRA) is a public repository that serves as a centralized archive for storing and sharing high-throughput sequencing data. Its primary goal is to facilitate the sharing and accessibility of raw sequence data produced by various genomics and transcriptomics experiments. The SRA has been essential for my doctoral studies, as I have relied solely on public data. For \textbf{Chapter 6}, I aimed to download all available human H3K27ac data (+/- 12,000 samples) from the SRA as a reference database. Processing these samples meant that I had to download 20TB from the SRA and spent approximately 300,000 CPU hours processing them on Cartesius, costing roughly €1,000 for the SRA\cite{amazon}, €9,000 for Cartesius\cite{cartesius}, and emitting nearly one tonne of CO2\cite{CO2}. In the analysis that followed, however, it turned out that obtaining sample annotations, such as tissue or cell type, is challenging due to the lack of standardized metadata on the SRA. Even though third-party tools like MetaSRA\cite{Bernstein2017}, and PredictMEE\cite{Klie2021} have been developed to automatically infer this metadata, they did not manage to resolve my issues. In the end, I decided to discard the experiment, wasting an enormous amount of resources.

Not only is the SRA missing crucial metadata, but their sra-tools, a toolkit for downloading sequencing data, are also particularly difficult to use. As a consequence, many alternative tools have been developed for downloading data from the SRA, such as the SRA-explorer\cite{sraexplorer}, pysradb\cite{pysradb}, FetchFastQ\cite{galvez2022metadata}, nf-core/fetchngs\cite{fetchngs}, parallel-fastq-dump\cite{parallelfastq}, and finally our download-fastq workflow of seq2science\cite{seq2science} (\textbf{Chapter 3}). The poor implementation of the sra-tools results in additional and unnecessary work, costing time and resources of third-party researchers. Similarly, the submission of new sequencing data is notoriously complicated, causing people to make mistakes, which even leads to others developing tools to streamline this process\cite{Quiones2020}. 

MACS2 is one of the most popular tools in bioinformatics and is used to discover enriched regions in the genome, and has been cited more than 14,000 times\cite{Zhang2008}. MACS2 has been continuously developed since 2008, and its developers should get credit for this effort. Nevertheless, MACS2 is a particularly poorly designed and unoptimized software tool. The recommended way to use MACS2 in the case of ATAC-seq discards all mates from paired-end data, effectively removing half of the cleavage information\cite{Gaspar2018}. This can be overcome by removing the mate tag of reads after alignment, or by converting the bam alignment file to a bed and using that as input. Most users are however not aware of this unexpected implementation, resulting in suboptimal sets of peaks in presumably the majority of ATAC-seq studies that use MACS2. As a practically identical, but better alternative to MACS2, Genrich has been developed by an independent group\cite{genrich}. 

Finally, the quickest developing part of molecular analyses currently is the field of single-cell analyses. As a consequence, the most technical debt is taken in these parts. Single-cell tools are usually implemented poorly, taking days to run and requiring enormous amounts of memory\cite{Pratapa_2020}. The usual solution to this problem is not to improve the software implementation, but instead to increase the hardware capabilities. For instance, an ex-colleague of mine requested a massive 4TB of RAM for his computer at his new position. A particularly painful point of technical debt in the single-cell field is the implementation of the log fold change calculation of Seurat. Seurat calculates the log of the means (as opposed to the mean of the logs), which makes it sensitive to outliers and this implementation should be corrected. This results in vastly different outcomes depending on whether one used Seurat or any alternative tool such as Scanpy for their analysis. The bug was reported in November 2022, got an incomplete fix in March 2023, and as yet has not been fixed (https://github.com/satijalab/seurat/issues/6654). Seurat has been cited 7,699 times.

Researchers have to develop new tools and analyses, whilst it is not even clear whether those approaches will be successful. In these cases taking \textit{deliberate} technical debt is okay. The problem with bioinformatics is, however, that a large part of the technical debt is taken inadvertently. I think the two main reasons for this are the lack of formal bioinformatics training (as mentioned in \textbf{chapter 2}), and the lack of interdepartmental software collaborations. Two things that, in hindsight, were also missing during my doctoral studies. 

Because the field of bioinformatics is young and rapidly becoming more important the supply of bioinformaticians is lacking behind their demand. This in turn leads to biologists starting bioinformatics analyses without formal training. This is somehow a widely accepted fact in the field, and even facilitated and encouraged by projects such as Galaxy\cite{galaxy}. In turn, bioinformaticians without formal training in programming, statistics, or modeling are developing tools and file formats, and doing analyses, which simply means that the quality and correctness of these tools and analyses are low. A clear example is the widespread use of Excel, which notoriously mangles gene names\cite{Zeeberg2004}. For example, the gene SEPT1 gets converted into September 1, and consequently, approximately $30\%$ of studies report these mangled gene names in their supplementary data\cite{Abeysooriya2021}. Likewise, as far as I am aware all the first authors of the studies I re-analyzed in \textbf{Chapters 4 and 5} are primarily trained as biologist, not as computational biologists. It is surprising that the idea prevails that anyone should be able to do a bioinformatics analysis, whilst wet lab work is typically restricted to those who have undergone formal training. By not formally training the current and next generation of bioinformatics, we are hindering long-term progress in the field.

Another part of the problem is that most tools are developed independently, most of the time by a single doctoral student or postdoc. This has two major downsides. The first is that the maintenance of tools depends on individuals. If someone decides to leave academia, their tools, how useful they might be, will soon become obsolete as no one will maintain them. A clear example is the dissolution of the \say{van Heeringen} group, where soon tools such as genomepy\cite{genomepy}, gimmemotifs\cite{Bruse_2018}, ananse\cite{Xu_2020}, scepia, seq2science\cite{seq2science}, and qnorm\cite{qnorm} will become obsolete as they will no longer be maintained. Second, most tools are unnecessarily specialized for the task the developer is trying to solve. This in turn leads to many near-identical implementations that solve the same problem, for example, the many similar SRA-fastq downloading implementations mentioned before, including the seq2science implementation. These problems could be overcome if bioinformaticians would develop more interdepartmental collaborations. A software collaboration between multiple groups and expertise levels would ensure projects are not abandoned when a person leaves and that tools are useful for wider audiences. Moreover, working in close collaborations has the advantage that people learn from one another.

In summary, despite the remarkable scientific breakthroughs facilitated by bioinformatics in the past decades, the field is confronted with an increasingly taxing technical debt. This accumulating burden reduces the efficiency of current analyses, increases the risk of errors, and demands urgent attention. Addressing bioinformatics' technical debt requires a normalization of formal training and interdisciplinary collaborations - failure to do so risks inhibiting future advancements in the field.

\section{Concluding Remarks}

The transcription regulation of a gene is a highly complex process, which depends on numerous factors such as chromatin context and transcription factor gene expression. The regulation of transcription is dynamic and changes throughout (embryonic) development, and is prone to changes through evolution. To study this phenomenon, experimental biologists have generated enormous amounts of data, which is typically analyzed by bioinformaticians and computational biologists. In this thesis I propose three improvements to gene regulatory network inference; the use of multiple omics, the use of single-cell data over bulk sequencing, and the use of universal models. Additionally, I introduce two computational tools that aid in the investigation of transcription regulation. Seq2science is an end-to-end functional genomics preprocessing tool, and SCEPIA is a tool that facilitates the linking of single-cell transcriptomic information to an extensive database of chromatin context to improve motif activity inference. Furthermore, in this thesis, I present my findings on the phylotypic stage, a stage that is supposedly highly conserved between species. I explain how its many definitions are ambiguous, which leads to unsupported conclusions from the data. Altogether, this thesis provides computational perspectives on transcription regulation throughout evolution and development. 
